{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession. No need to create SparkContext\n",
    "# You automatically get it as part of the SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ETL\").getOrCreate()\n",
    "\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\n",
    "# now we can go to http://localhost:4040 (default port) in order to see Spark's web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring Sparkâ€™s Runtime Properties\n",
    "esto es java o scala\n",
    "\n",
    "//set new runtime options\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "//get all settings\n",
    "val configMap:Map[String, String] = spark.conf.getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Datasets and Dataframes\n",
    "\n",
    "#create a Dataset using spark.range starting from 5 to 100, with increments of 5\n",
    "val numDS = spark.range(5, 100, 5)\n",
    "# reverse the order and display first 5 items\n",
    "numDS.orderBy(desc(\"id\")).show(5)\n",
    "#compute descriptive stats and display them\n",
    "numDs.describe().show()\n",
    "# create a DataFrame using spark.createDataFrame from a List or Seq\n",
    "val langPercentDF = spark.createDataFrame(List((\"Scala\", 35), (\"Python\", 30), (\"R\", 15), (\"Java\", 20)))\n",
    "#rename the columns\n",
    "val lpDF = langPercentDF.withColumnRenamed(\"_1\", \"language\").withColumnRenamed(\"_2\", \"percent\")\n",
    "#order the DataFrame in descending order of percentage\n",
    "lpDF.orderBy(desc(\"percent\")).show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the json file and create the dataframe\n",
    "val jsonFile = args(0)\n",
    "val zipsDF = spark.read.json(jsonFile)\n",
    "# filter all cities whose population > 40K\n",
    "zipsDF.filter(zipsDF.col(\"pop\") > 40000).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasetDir = \"../../datasets/\" # local files\n",
    "datasetDir = \"hdfs://localhost:19000/\" # hadoop filesystem\n",
    "\n",
    "file = datasetDir + \"HiggsTwitter/higgs-social_network.edgelist.gz\"\n",
    "schema = StructType([StructField(\"follower\", IntegerType()), StructField(\"followed\", IntegerType())])\n",
    "socialDF = spark.read.csv(path=file, sep=\" \", schema=schema)\n",
    "\n",
    "file = datasetDir + \"HiggsTwitter/higgs-retweet_network.edgelist.gz\"\n",
    "schema = StructType([StructField(\"tweeter\", IntegerType()), StructField(\"tweeted\", IntegerType()), StructField(\"occur\", IntegerType())])\n",
    "retweetDF = spark.read.csv(path=file, sep=\" \", schema=schema)\n",
    "\n",
    "file = datasetDir + \"HiggsTwitter/higgs-reply_network.edgelist.gz\"\n",
    "schema = StructType([StructField(\"replier\", IntegerType()), StructField(\"replied\", IntegerType()), StructField(\"occur\", IntegerType())])\n",
    "replyDF = spark.read.csv(path=file, sep=\" \", schema=schema)\n",
    "\n",
    "file = datasetDir + \"HiggsTwitter/higgs-mention_network.edgelist.gz\"\n",
    "schema = StructType([StructField(\"mentioner\", IntegerType()), StructField(\"mentioned\", IntegerType()), StructField(\"occur\", IntegerType())])\n",
    "mentionDF = spark.read.csv(path=file, sep=\" \", schema=schema)\n",
    "\n",
    "file = datasetDir + \"HiggsTwitter/higgs-activity_time.txt.gz\"\n",
    "schema = StructType([StructField(\"userA\", IntegerType()), \\\n",
    "                     StructField(\"userB\", IntegerType()), \\\n",
    "                     StructField(\"timestamp\", IntegerType()), \\\n",
    "                    StructField(\"interaction\", StringType())])\n",
    "                    #Interaction can be RT (retweet), MT (mention) or RE (reply)\n",
    "activityDF = spark.read.csv(path=file, sep=\" \", schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- follower: integer (nullable = true)\n",
      " |-- followed: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(follower,IntegerType,true),StructField(followed,IntegerType,true)))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socialDF.printSchema()\n",
    "\n",
    "socialDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|follower|followed|\n",
      "+--------+--------+\n",
      "|       1|       2|\n",
      "|       1|       3|\n",
      "|       1|       4|\n",
      "+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-------+-----+\n",
      "|tweeter|tweeted|occur|\n",
      "+-------+-------+-----+\n",
      "| 298960| 105232|    1|\n",
      "|  95688|   3393|    1|\n",
      "| 353237|  62217|    1|\n",
      "+-------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-------+-----+\n",
      "|replier|replied|occur|\n",
      "+-------+-------+-----+\n",
      "| 161345|   8614|    1|\n",
      "| 428368|  11792|    1|\n",
      "|  77904|  10701|    1|\n",
      "+-------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+---------+-----+\n",
      "|mentioner|mentioned|occur|\n",
      "+---------+---------+-----+\n",
      "|   316609|     5011|    1|\n",
      "|   439696|    12389|    1|\n",
      "|    60059|     6929|    1|\n",
      "+---------+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+------+----------+-----------+\n",
      "| userA| userB| timestamp|interaction|\n",
      "+------+------+----------+-----------+\n",
      "|223789|213163|1341100972|         MT|\n",
      "|223789|213163|1341100972|         RE|\n",
      "|376989| 50329|1341101181|         RT|\n",
      "+------+------+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "socialDF.show(3)\n",
    "retweetDF.show(3)\n",
    "replyDF.show(3)\n",
    "mentionDF.show(3)\n",
    "activityDF.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|follower|follows|\n",
      "+--------+-------+\n",
      "|   13115|   1259|\n",
      "|   49180|   1155|\n",
      "|   50338|   1097|\n",
      "|    1984|   1093|\n",
      "|    3628|   1027|\n",
      "+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+---------+\n",
      "|followed|followers|\n",
      "+--------+---------+\n",
      "|    1503|    51386|\n",
      "|     206|    48414|\n",
      "|      88|    45221|\n",
      "|     138|    44188|\n",
      "|    1062|    40120|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# User who follows most users\n",
    "socialDF.groupBy(\"follower\").agg(count(\"followed\").alias(\"follows\")).orderBy(desc(\"follows\")).show(5)\n",
    "\n",
    "# User who has most followers\n",
    "socialDF.groupBy(\"followed\").agg(count(\"followed\").alias(\"followers\")).orderBy(desc(\"followers\")).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL using SQL language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14855842"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "socialDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|follower|follows|\n",
      "+--------+-------+\n",
      "|   13115|   1259|\n",
      "|   49180|   1155|\n",
      "|   50338|   1097|\n",
      "|    1984|   1093|\n",
      "|    3628|   1027|\n",
      "+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+---------+\n",
      "|followed|followers|\n",
      "+--------+---------+\n",
      "|    1503|    51386|\n",
      "|     206|    48414|\n",
      "|      88|    45221|\n",
      "|     138|    44188|\n",
      "|    1062|    40120|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#socialDF.createOrReplaceTempView(\"social\")\n",
    "\n",
    "# User who follows most users\n",
    "spark.sql(\"select follower, count(followed) as follows from social group by follower order by count(followed) desc\").show(5)\n",
    "\n",
    "# User who has most followers\n",
    "spark.sql(\"select followed, count(follower) as followers from social group by followed order by count(follower) desc\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "appName = \"miApp\"\n",
    "master=\"local[*]\"\n",
    "\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf) # start the SparkContext\n",
    "\n",
    "# now we can go to http://localhost:4040 (default port) in order to see Spark's web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "textFile = sc.textFile(\"../../datasets/worldbank-millennium-development-goals.csv\")\n",
    "\n",
    "# datasets at:\n",
    "#https://datasource.kapsarc.org/explore/dataset/worldbank-millennium-development-goals/export/\n",
    "#https://datasource.kapsarc.org/explore/dataset/worldbank-health-nutrition-and-population-statistics/export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "fSocial = sc.textFile(\"../../datasets/HiggsTwitter/higgs-social_network.edgelist.gz\")\n",
    "fRetweet = sc.textFile(\"../../datasets/HiggsTwitter/higgs-retweet_network.edgelist.gz\")\n",
    "fReply = sc.textFile(\"../../datasets/HiggsTwitter/higgs-reply_network.edgelist.gz\")\n",
    "fMention = sc.textFile(\"../../datasets/HiggsTwitter/higgs-mention_network.edgelist.gz\")\n",
    "fActivity = sc.textFile(\"../../datasets/HiggsTwitter/higgs-activity_time.txt.gz\")\n",
    "\n",
    "# datasets at http://snap.stanford.edu/data/higgs-twitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fSocial.take(3))\n",
    "print(fRetweet.take(3))\n",
    "print(fReply.take(3))\n",
    "print(fMention.take(3))\n",
    "print(fActivity.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fSocial.take(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "social = sqlContext.load(path = 'PATH/train.csv', header = True,inferSchema = True)\n",
    "test = sqlContext.load(source=\"com.databricks.spark.csv\", path = 'PATH/test-comb.csv', header = True,inferSchema = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.createDataFrame(fSocial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textFile.count()) # Number of items in this RDD\n",
    "\n",
    "textFile.first()  # First item in this RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.filter(lambda line: \"Spain\" in line).count()  # Lines containing \"Spain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the line with the most words (lambda format)\n",
    "textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the line with the most words (function format)\n",
    "def max(a, b):\n",
    "     if a > b:\n",
    "         return a\n",
    "     else:\n",
    "         return b\n",
    "\n",
    "\n",
    "textFile.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:\n",
    "wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.take(10)\n",
    "#wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textFile.first() # returns first item\n",
    "textFile.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.map(lambda x: (x.split(';'))).take(3) # returns 1 RDD containing 3 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.flatMap(lambda x: (x.split(';'))).take(8) # returns 8 separated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo que funciona:\n",
    "fieldcount = textFile.flatMap(lambda line: line.split(';')).map(lambda field: (field, 1)).reduceByKey(lambda a, b: a+b)\n",
    "wordCounts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Year\n",
    "years = lines.map(lambda x: (x[0].split(';')[4], 1))\n",
    "years.take(5)\n",
    "#counts = years.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#print(counts.collect()) # TODO: fix it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXAMPLE\n",
    "pairs = people.map(lambda x: (x[4], 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
